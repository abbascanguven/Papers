# Papers

Papers List

1- Attention is ALl you Need
2- BERT: Pre-training of Deep Bidirectional Transformers for
3- ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS
